{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ce0eae9-c866-46ce-84cb-22a3d85d23c4",
   "metadata": {},
   "source": [
    "### What is A/B testying?\n",
    "- A/B testing is a tool that allows you to test two or more different ideas against each other in the real world, and to see which one empirically performs better.\n",
    "- **A/B Testing:** Test different ideas against each other in the real world\n",
    "- Choose the one that statistically better\n",
    "### Why is A/B testing important?\n",
    "- No guessing\n",
    "- Providees accurate answers - quickly\n",
    "- ...and establish causal relationships\n",
    "\n",
    "### A/B test process\n",
    "- Develop a hypothesis about your product or business\n",
    "- **Randomly** assign users to two different groups\n",
    "- Expose:\n",
    "    - Group 1 to the current product rules\n",
    "    - Group 2 to a product that tests the hypothesis\n",
    "- Pick whichever performs better accordingto a set of KPIs (Key performance indicators)\n",
    "\n",
    "### Where can A/B testing be used?\n",
    "Users + ideas -> A/B test\n",
    "- testing impact of drugs\n",
    "- incentivizing spending\n",
    "- driving user growth\n",
    "- as well as many more use cases beyond these\n",
    "\n",
    "### Key Performance Indicators (KPIs)\n",
    "- **A/B Tests:** Measure impact of changes on KPIs\n",
    "- **KPIs** - metrics important to an organization\n",
    "    - likelihood of a side-effect\n",
    "    - revenue\n",
    "    - conversion rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a01478-a721-4f91-87fc-58a1878bedb3",
   "metadata": {},
   "source": [
    "### Pandas Merge Method\n",
    "- Merging - equivalent of SQL `JOIN`\n",
    "- In `pandas`:\n",
    "    - `pd.merge(df1,df2)`\n",
    "    - `df1.merge(df2)`\n",
    "    \n",
    "### Merging mechganics\n",
    "df_new = df1.merge(df2, how='inner', on=['id'])\n",
    "\n",
    "### Group: `pandas.DataFrame.groupby()`\n",
    "DataFrame.groupby(by=$\\color{red}{\\text{None}}$, axis=$\\color{red}{\\text{0}}$, level=$\\color{red}{\\text{None}}$\n",
    "<br>&emsp;&emsp;&emsp;&emsp; as_index=$\\color{red}{\\text{True}}$, sort=$\\color{red}{\\text{True}}$,\n",
    "<br>&emsp;&emsp;&emsp;&emsp; group_keys=$\\color{red}{\\text{True}}$, squeez=$\\color{red}{\\text{False}}$, **kwargs)\n",
    "            \n",
    "### Aggregate: `pandas.DataFrame.agg()`\n",
    "DataFrame.agg(func, axis=$\\color{red}{\\text{0}}$, *args, **kwargs)\n",
    "\n",
    "### Grouping Data: .groupby()\n",
    "- `by`: fields to group by\n",
    "- `axis`: `axis=0` will group by columns, `axis=1` will group by rows\n",
    "- `as_index`: `as_index=True` will use group labels as index\n",
    "\n",
    "df_groupby = df1.groupby(by=['$\\color{red}{\\text{country}}$' , '$\\color{red}{\\text{device}}$'],\n",
    "axis=$\\color{red}{\\text{0}}$, as_index=$\\color{red}{\\text{False}}$)\n",
    "\n",
    "Let us group by \"country\" and \"device\". The next relevant argument is `axis` which specifies whether we are grouping by row or column values. The default value, \"0\", groups by columns. The other argument of interest is `as_index`. By default, this argument is “True”, which means that the grouped by fields become indices. We want to set this to \"False\" so that this does not happen. This returns a dataFrameGroupedBy object. \n",
    "\n",
    "#### Aggregatng data\n",
    "df_groupby.price.mean()\n",
    "\n",
    "#### Aggregate data: .agg()\n",
    "Pass the name of an aggregation function to `agg()`\n",
    "<br> df_groupby.price.agg('$\\color{red}{\\text{mean}}$')\n",
    "<br> df_groupby.price.agg(['$\\color{red}{\\text{mean}}$' , '$\\color{red}{\\text{median}}$'])\n",
    "\n",
    "#### .agg(): multiple functions, multiple columns\n",
    "Pass a dictionary of column names and aggregation functions\n",
    "<br> df_groupby.agg({'price' : ['$\\color{red}{\\text{mean}}$' , '$\\color{red}{\\text{min}}$' , '$\\color{red}{\\text{max}}$'],\n",
    "<br>&emsp;&emsp;&emsp;&emsp; 'age': ['$\\color{red}{\\text{mean}}$' , '$\\color{red}{\\text{min}}$' , '$\\color{red}{\\text{max}}$']})\n",
    "\n",
    "#### .agg(): custom functions\n",
    "$\\color{green}{\\text{def}}$ $\\color{red}{\\text{truncate_mean}}$(data):\n",
    "<br>&emsp;&emsp; $\\color{red}{\\text{\"\"\"Compute the mean excluding outliers\"\"\"}}$\n",
    "<br>&emsp;&emsp; top_val = data.quantile($\\color{red}{\\text{0.9}}$)\n",
    "<br>&emsp;&emsp; bot_val = data.quantile($\\color{red}{\\text{0.1}}$)\n",
    "<br>&emsp;&emsp; trunc_data = data[(data <= top_val) & (data >= bot_val)]\n",
    "<br>&emsp;&emsp; mean = trunc_data.mean()\n",
    "<br>&emsp;&emsp; $\\color{green}{\\text{return}}$(mean)\n",
    "<br>\n",
    "<br> df_groupby.agg({'$\\color{red}{\\text{age}}$' : [truncate_mean]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac84175-517a-4f1c-82c4-56e1a4bb36c4",
   "metadata": {},
   "source": [
    "#### Using the Timedelta class\n",
    "- **Lapse Date**: Date the trial ends for a given user\n",
    "\n",
    "#### Parsing dates - on import\n",
    "pandas.read_csv(....,\n",
    "<br>&emsp;&emsp; parse_dates=$\\color{red}{\\text{False}}$,\n",
    "<br>&emsp;&emsp; infer_datetime_format=$\\color{red}{\\text{False}}$,\n",
    "<br>&emsp;&emsp; keep_date_col=$\\color{red}{\\text{False}}$,\n",
    "<br>&emsp;&emsp; date_parser=$\\color{red}{\\text{None}}$,\n",
    "<br>&emsp;&emsp; dayfirstr=$\\color{red}{\\text{False}}$, ....)\n",
    "\n",
    "#### Example:\n",
    "df = pd.read_csv('file.csv',\n",
    "<br>&emsp;&emsp; parse_dates=$\\color{red}{\\text{True}}$,\n",
    "<br>&emsp;&emsp; infer_datetime_format=$\\color{red}{\\text{True}}$)\n",
    "\n",
    "\n",
    "Two of the primary arguments, `parse_dates` and `infer_datetime_format` are shown in use above. By setting these to true, `read_csv()` will attempt to convert the string representation to a date on import.\n",
    "\n",
    "#### Parsing dates - manually\n",
    "`pandas.to_datetime(arg, errors='raise', ..., format=None, ...)`\n",
    "\n",
    "##### strftime \n",
    "Python strftime cheatsheet: \n",
    "https://strftime.org\n",
    "- 1993-01-27 -- `\"%Y-%m-%d\"`\n",
    "- 05/13/2017 05:45:37 -- `\"%m/%d/%Y %H:%M:%S\"`\n",
    "- September 01, 2017 -- `\"%B %d, %Y\"`\n",
    "\n",
    "#### Pivot table\n",
    "pandas.pivot_table($\\color{red}{\\text{data}}$, values=$\\color{red}{\\text{None}}$, index=$\\color{red}{\\text{None}}$, columns=$\\color{red}{\\text{None}}$, aggfunc='$\\color{red}{\\text{mean}}$', <br>&emsp;&emsp; fill_value=$\\color{red}{\\text{None}}$, margins=$\\color{red}{\\text{False}}$, dropna=$\\color{red}{\\text{True}}$, margins_name='$\\color{red}{\\text{All}}$', observed=$\\color{red}{\\text{False}}$, sort=$\\color{red}{\\text{True}}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95312636-bd35-4da3-b1d5-1d2d079d86f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>foo</td>\n",
       "      <td>one</td>\n",
       "      <td>small</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>foo</td>\n",
       "      <td>one</td>\n",
       "      <td>large</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>foo</td>\n",
       "      <td>one</td>\n",
       "      <td>large</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>foo</td>\n",
       "      <td>two</td>\n",
       "      <td>small</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>foo</td>\n",
       "      <td>two</td>\n",
       "      <td>small</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bar</td>\n",
       "      <td>one</td>\n",
       "      <td>large</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bar</td>\n",
       "      <td>one</td>\n",
       "      <td>small</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bar</td>\n",
       "      <td>two</td>\n",
       "      <td>small</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bar</td>\n",
       "      <td>two</td>\n",
       "      <td>large</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B      C  D  E\n",
       "0  foo  one  small  1  2\n",
       "1  foo  one  large  2  4\n",
       "2  foo  one  large  2  5\n",
       "3  foo  two  small  3  5\n",
       "4  foo  two  small  3  6\n",
       "5  bar  one  large  4  6\n",
       "6  bar  one  small  5  8\n",
       "7  bar  two  small  6  9\n",
       "8  bar  two  large  7  9"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# \n",
    "df = pd.DataFrame({\"A\": [\"foo\", \"foo\", \"foo\", \"foo\", \"foo\",\n",
    "                         \"bar\", \"bar\", \"bar\", \"bar\"],\n",
    "                   \"B\": [\"one\", \"one\", \"one\", \"two\", \"two\",\n",
    "                         \"one\", \"one\", \"two\", \"two\"],\n",
    "                   \"C\": [\"small\", \"large\", \"large\", \"small\",\n",
    "                         \"small\", \"large\", \"small\", \"small\",\n",
    "                         \"large\"],\n",
    "                   \"D\": [1, 2, 2, 3, 3, 4, 5, 6, 7],\n",
    "                   \"E\": [2, 4, 5, 5, 6, 6, 8, 9, 9]})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "432f38c6-19ef-4e90-a41f-3014b950f72a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>large</th>\n",
       "      <th>small</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">bar</th>\n",
       "      <th>one</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">foo</th>\n",
       "      <th>one</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "C        large  small\n",
       "A   B                \n",
       "bar one      4      5\n",
       "    two      7      6\n",
       "foo one      4      1\n",
       "    two      0      6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.pivot_table(df, values='D', index=['A', 'B'],\n",
    "                    columns=['C'], aggfunc=np.sum, fill_value=0)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4e44801-cd54-4816-bb43-83ab882a4ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 <class 'int'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2],\n",
       "       [ 0,  2, -1],\n",
       "       [ 0,  3, -1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "# \n",
    "# Define the most recent date in our  data\n",
    "current_date = pd.to_datetime('2018-03-17')\n",
    "# The last date a user could lapse be included\n",
    "max_lapse_date = current_date - timedelta(days=14)\n",
    "# \n",
    "sub_time = current_date - max_lapse_date\n",
    "# convert the sub_time from a timedelta to an int\n",
    "sub_time = sub_time.days\n",
    "print(sub_time, type(sub_time))\n",
    "# \n",
    "a = np.array([[0, 1, 2],\n",
    "              [0, 2, 4],\n",
    "              [0, 3, 6]])\n",
    "np.where(a < 4, a, -1)  # -1 is broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23860136-3c36-403f-a192-1c58d05c1b5c",
   "metadata": {},
   "source": [
    "#### Correcting for seasonality with trailing averages\n",
    "- We can correct for this movement, by calculating a trailing average over our data. A trailing average is a smoothing technique that sets the value for a given day as the average over the past n-days. To smooth weekly seasonality we want n-equals-seven. This has the effect of averaging over a week such that every day is pulled towards the weekly level limiting the day level effects.\n",
    "- First, we use the pandas `rolling()` method to find the rolling window. The primary parameters of `rolling()` are `window` which is our n value from above, and `center` which is a Boolean. If this is true then our averaged value will be placed at the middle of our window, if not it will be placed in the index that we are looking back from, which is what we want.\n",
    "- Once we have this we call `mean` to find our average over the window. We can calculate this and store it in an additional variable.\n",
    "\n",
    "###### Example:\n",
    "- series_new = df.column.rolling(window=$\\color{red}{\\text{7}}$, center=$\\color{red}{\\text{False}}$)\n",
    "    - window: how many data points to average over\n",
    "    - center: specify to average backwards\n",
    "- df['$\\color{red}{\\text{rolling_column}}$'] = series_new.mean()\n",
    "\n",
    "#### Noisy data\n",
    "- It is incredibly noisy, as the values vary widely from day-to-day. We can apply an exponential moving average so we can check if any macro trends are hidden among this noise.\n",
    "- This type of average weights the points such that the earlier ones are weighted less than the more recent ones within our window. This pulls our data back to any central trend, while maintaining any recent movements.\n",
    "- We can use the `ewm()` method to find these weighted windows. To do this we specify the `span` argument to be our window size. Then we find the average of this weighted data. Here we apply this to our set of purchase data, using a 30-day window. Determining windows like this can require prior knowledge of the structure of the data or some trial-and-error. See that this does remove a lot of the noise and reveals a slight upward trend.\n",
    "    - `.ewm()`: exponential weighting function\n",
    "    - `span`: Window to apply weights over\n",
    "\n",
    "###### Example:\n",
    "exp_mean = df.column.ewm(span=$\\color{red}{\\text{30}}$)\n",
    "df['$\\color{red}{\\text{exp_mean}}$'] = exp_mean.mean()\n",
    "\n",
    "\n",
    "#### Summary - Data Smoothing Techniques\n",
    "- **Trailing Average**:\n",
    "    - Smooths seasonality by averaging over the periodicity\n",
    "- **Exponential Moving Average**:\n",
    "    - Reveals trend by pulling towards the central tendency\n",
    "    - Weights the most recent values relative to the window more heavily\n",
    "- You can use `.rolling()` and `.ewm()` for many more methods of smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56352d3d-2bc6-4493-ad14-e48f37132764",
   "metadata": {},
   "source": [
    "#### Plotting annotations - events\n",
    "- `plt.axvline()`: Plots vertical line at x-intercept\n",
    "    - `color`: Specify the color of the plotted line\n",
    "    - `linestyle`: The type of line to plot\n",
    "    \n",
    "##### Example:\n",
    "- df.plot(x=['$\\color{red}{\\text{column1}}$'], y=['$\\color{red}{\\text{label_1}}$', '$\\color{red}{\\text{label_2}}$'])\n",
    "- events.Date = pd.to_datetime(events.Date)\n",
    "- $\\color{blue}{\\text{for}}$ row $\\color{blue}{\\text{in}}$ events.iterrows():\n",
    "- &emsp;&emsp; tmp = row[$\\color{red}{\\text{1}}$]\n",
    "- &emsp;&emsp; plt.axvline(\n",
    "- &emsp;&emsp; x=tmp.Datew, color=$\\color{red}{\\text{'k'}}$, linestyle=$\\color{red}{\\text{'--'}}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca6555b-63c8-4d2b-b2a2-3caa973b3c78",
   "metadata": {},
   "source": [
    "### Introduction to A/B testing\n",
    "- An A/B test is an experiment in which you test two different values of the same variable against one another to determine which one is “better” by evaluating their performance among two randomly assigned groups of real users.\n",
    "- A/B testing is a powerful and flexible tool. You could apply it to improve sales or interactions within an app or website, or in a variety of other contexts form medicine (called randomized controlled trials in this case) to optimizing efficiency on an assembly line.\n",
    "- With that said it is important to be aware of the cases that are and are not conducive to improvement through A/B testing. While no general rule exists, there are some things worth considering. Good problems for A/B testing are those where users are being impacted in an individual way. Questions like which color will make the user more likely to press a button or which price is better are ripe for A/B testing.\n",
    "- Bad problems for A/B testing are those in which there are network effects of users, that is the change you want to test impacts or is directly related to an area where users are interacting with one another. An example of this is the matching algorithm for a dating app. It is hard to untangle the impact of the change if users are matching with people outside of the test group. There are many nuances to this, but it is worth thinking through how directly you can connect the change made in the A/B test to the change in behaviors. If you can't do this well, perhaps consider other methods.\n",
    "\n",
    "#### Considerations in test design\n",
    "- There are two primary concerns in test design. Ensuring that our test can be practically run and that we can derive meaningful results from it. These two objectives are strongly connected.\n",
    "\n",
    "#### Test sensitivity\n",
    "- A good starting point is to ask, what percentage change would it be meaningful to detect in your response variable. 1% - 20%? It makes sense that smaller changes would be more difficult to detect, as they can more easily be overshadowed by randomness. The minimum level of change we want to detect is called __sensitivity__.\n",
    "\n",
    "#### Data variability\n",
    "- While understanding the desired change in the data due to the treatment is important, it is also important to understand the latent variability in the data. In this case, it makes sense to understand if the purchase amount is consistent across all users, or if it varies widely. A change due to the treatment will be more easily captured in the former case.\n",
    "\n",
    "#### Standard deviation\n",
    "- We can find the standard deviation of our data using the pandas `std()` method by passing in a vector of our statistics. Typically, we will rely on the standard deviation of the test results in evaluating our test, but using the value of our initial data is important for planning as we will see.\n",
    "\n",
    "#### Null hypothesis\n",
    "- This is the hypothesis that our control and treatment, that is our two phrases, have the same impact on the response. Any observed difference is just due to randomness. If we can conclude this is not the case, then we say our results are statistically significant and that there is a difference.\n",
    "    - A Type I error (or Type 1), is the incorrect rejection of a true null hypothesis. ($\\alpha$)\n",
    "        - Reject the null hypothesis when there is in fact no significant effect (false positive). The `p-value` is optimistically small.\n",
    "    - A Type II error (sometimes called a Type 2 error) is the failure to reject a false null hypothesis. ($\\beta$)\n",
    "        - Not reject the null hypothesis when there is a significant effect (false negative). The `p-value` is pessimistically large. \n",
    "    - In interpreting the p-value of a significance test, you must specify a significance level, often referred to as the Greek lower case letter alpha ($\\alpha$). A common value for the significance level is 5% written as 0.05.\n",
    "    - The `p-value` is interested in the context of the chosen significance level. A result of a significance test is claimed to be “statistically significant” if the `p-value` is less than the significance level. This means that the null hypothesis (that there is no result) is rejected\n",
    "    - The `p-value` is the probability of observing a value as or more extreme than the one observed under the Null Hypothesis.\n",
    "            - p <= alpha: reject Ho, different distribution.\n",
    "            - p > alpha: fail to reject Ho, same distribution.\n",
    "\n",
    "#### Types of error & confidence level\n",
    "- Rejecting the null hypothesis when it is true is called type I error, and retaining the false null hypothesis is type II error. We define our probability of not making a type I error as the Confidence Level. We will not go into great detail, but intuitively it should make sense that the higher we make this value the larger of a sample we will need. A common value of this is 0 point 95\n",
    "\n",
    "#### Statistical power\n",
    "- Related to this is the idea of Statistical Power. Power is the probability of finding statistically significant results when the Null hypothesis is false.\n",
    "\n",
    "####  Connecting the Different Components\n",
    "- Power and Confidence level are connected to the standard error and sensitivity of our test. To estimate our needed sample size, we can choose our desired sensitivity, set our desired confidence level & power, and then estimate our standard error using these values.\n",
    "\n",
    "#### Power formula\n",
    "- The key takeaway to note is that the relation between Power and n, our sample size, is that as n goes up so too does our power. Additionally, as our confidence level goes up our power goes down.\n",
    "\n",
    "- What is Power?\n",
    "    - Power is how likely there is an actual effect vs. chance.\n",
    "    - Likelihood that the test is correctly \"proving\" your hypotheses\n",
    "    - A high power means that the test results are likely valid. As the power increases, the probability of making a Type II error decreases.\n",
    "    - A low power meansthat the test results are questionable\n",
    "    - Helps determine if sample size is large enough.\n",
    "    - The statistical power of a study (sometimes called sensitivity) is how likely the study is to distinguish an actual effect from one of chance. It’s the likelihood that the test is correctly rejecting the null hypothesis (i.e. “proving” your hypothesis). For example, a study that has an 80% power means that the study has an 80% chance of the test having significant results.\n",
    "    - It is possible to perform a hypothesis test without calculating the statistical power. If your sample size is too small, your results may be inconclusive when they may have been conclusive if you had a large enough sample.\n",
    "       \n",
    "#### Power Analysis\n",
    "- **Effect Size**: The quantified magnitude of a result present in the population. Effect size is calculated using a specific statistical measure, such as Pearson’s correlation coefficient for the relationship between variables or `Cohen’s d` for the difference between groups.\n",
    "- **Sample Size**: The number of observations in the sample.\n",
    "- **Significance**: The significance level used in the statistical test, e.g. alpha. Often set to 5% or 0.05.\n",
    "- **Statistical Power**: The probability of accepting the alternative hypothesis if it is true.\n",
    "\n",
    "All four variables are related. For example, a larger sample size can make an effect easier to detect, and the statistical power can be increased in a test by increasing the significance level. A power analysis involves estimating one of these four parameters given values for three other parameters. This is a powerful tool in both the design and in the analysis of experiments that we wish to interpret using statistical hypothesis tests.\n",
    "For example, the statistical power can be estimated given an effect size, sample size and significance level. Alternately, the sample size can be estimated given different desired levels of significance.\n",
    "\n",
    "We can start with sensible defaults for some parameters, such as a significance level of 0.05 and a power level of 0.80. We can then estimate a desirable minimum effect size, specific to the experiment being performed. A power analysis can then be used to estimate the minimum sample size required.\n",
    "\n",
    "In addition, multiple power analyses can be performed to provide a curve of one parameter against another, such as the change in the size of an effect in an experiment given changes to the sample size. More elaborate plots can be created varying three of the parameters. This is a useful tool for experimental design.\n",
    "\n",
    "##### Student’s t Test Power Analysis:\n",
    "- **Significance level (alpha)**: 5% or 0.05.\n",
    "- **Effect Size:** Cohen’s d of at least 0.80.\n",
    "- **Statistical Power:** 80% or 0.80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1128f6e0-b7ca-4931-a1c2-7067df68fe46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Size: 41.594\n"
     ]
    }
   ],
   "source": [
    "# estimate sample size via power analysis\n",
    "from statsmodels.stats.power import TTestIndPower\n",
    "# parameters for power analysis\n",
    "effect = 0.80\n",
    "alpha = 0.05\n",
    "power = 0.95\n",
    "# perform power analysis\n",
    "analysis = TTestIndPower()\n",
    "result = analysis.solve_power(effect, power=power, nobs1=None, ratio=1.0, alpha=alpha)\n",
    "print('Sample Size: %.3f' % result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f97f962a-1d65-41b5-a792-a8af8916e65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The statistical power is: 0.861422509233477\n",
      "We are 86% confident that we can reject the null hypothesis.\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "from statsmodels.stats.power import tt_ind_solve_power\n",
    "# \n",
    "effect_size=.8\n",
    "sample_size=30\n",
    "alpha=.05\n",
    "ratio=1.0\n",
    "statistical_power = tt_ind_solve_power(effect_size=effect_size, nobs1=sample_size, \n",
    "                                       alpha=alpha, \n",
    "                                       ratio=1.0, alternative='two-sided')\n",
    "print(\"The statistical power is: {0}\".format(statistical_power))\n",
    "print('We are 86% confident that we can reject the null hypothesis.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41924ed0-a973-4ff2-8059-be3427de7d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
