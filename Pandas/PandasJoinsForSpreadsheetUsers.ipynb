{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee85dc0f-2a19-4935-a5db-ddd2ca5d41ba",
   "metadata": {},
   "source": [
    "# Pandas Joins for Spreadsheet Users\n",
    "#### Split data\n",
    "- Influenced by reporting cycle\n",
    "- Common splits\n",
    "    - Time\n",
    "    - Geography\n",
    "    - Business unit\n",
    "\n",
    "- Complementary data\n",
    "    - Results from collecting data for different purposes\n",
    "    - Department-specific data\n",
    "    - Storage in separate files or database tables\n",
    "    \n",
    "- Concatenation basics\n",
    "    - Similar to spreadsheet CONCATENATE\n",
    "    - Mimics copy-paste of cells\n",
    "    - `pd.concat()` along rows or columns\n",
    "\n",
    "##### Concatenating rows\n",
    "Concatenating along rows is very useful when working with split data. For instance, reports are often produced annually with each year saved to a separate tab or file. You can vertically stack one or more data frames like these using pandas concat. The function will return a single data frame with data ordered by the order of the data frame names you supply, such as df1, df2, and so on. Basic concatenation works best when each input data frame has separate values for the index. That way the resulting frame will still have an index of unique values. Also, it’s worth noting that the concat function includes all rows by default. In other words, it creates an 'outer join'.\n",
    "- Useful when working with split data\n",
    "- `pd.concat([df1, df2, df3, ...])`\n",
    "- Uses unique key(s) as data frame index\n",
    "- Includes all rows by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4919e375-c774-4bc5-aac8-e25c6bfbc4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <th colspan=\"2\" halign=\"left\">one</th>\n",
       "      <th colspan=\"2\" halign=\"left\">two</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <th>dull</th>\n",
       "      <th>shiny</th>\n",
       "      <th>dull</th>\n",
       "      <th>shiny</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bar</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foo</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "b    one        two      \n",
       "c   dull shiny dull shiny\n",
       "a                        \n",
       "bar    1     2    1     0\n",
       "foo    2     2    1     2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# \n",
    "a = np.array([\"foo\", \"foo\", \"foo\", \"foo\", \"bar\", \"bar\",\"bar\", \n",
    "              \"bar\", \"foo\", \"foo\", \"foo\"], dtype=object)\n",
    "b = np.array([\"one\", \"one\", \"one\", \"two\", \"one\", \"one\",\"one\", \n",
    "              \"two\", \"two\", \"two\", \"one\"], dtype=object)\n",
    "c = np.array([\"dull\", \"dull\", \"shiny\", \"dull\", \"dull\", \"shiny\",\n",
    "              \"shiny\", \"dull\", \"shiny\", \"shiny\", \"shiny\"],\n",
    "             dtype=object)\n",
    "# \n",
    "pd.crosstab(a, [b, c], rownames=['a'], colnames=['b', 'c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922e6648-d058-4104-a4a0-88101a4d7e5d",
   "metadata": {},
   "source": [
    "Any Series passed will have their name attributes used unless row or column names for the cross-tabulation are specified.\n",
    "\n",
    "Any input passed containing Categorical data will have all of its categories included in the cross-tabulation, even if the actual data does not contain any instances of a particular category.\n",
    "\n",
    "In the event that there aren’t overlapping indexes an empty DataFrame will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd8bc2a2-2065-4f36-ab1e-c57fc43bf92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b']\n",
      "Categories (3, object): ['a', 'b', 'c']\n",
      "['d', 'e']\n",
      "Categories (3, object): ['d', 'e', 'f']\n"
     ]
    }
   ],
   "source": [
    "foo = pd.Categorical(['a', 'b'], categories=['a', 'b', 'c'])\n",
    "bar = pd.Categorical(['d', 'e'], categories=['d', 'e', 'f'])\n",
    "print(foo)\n",
    "print(bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c410c91-fd85-4dc9-8fef-9129fb33ff7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0  d  e\n",
       "row_0      \n",
       "a      1  0\n",
       "b      0  1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(foo, bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5984d17-535a-40bb-8900-89b224769b14",
   "metadata": {},
   "source": [
    "Here ‘c’ and ‘f’ are not represented in the data and will not be shown in the output because dropna is True by default. Set dropna=False to preserve categories with no data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0cfc2ca-ca0b-4223-8926-b18b420abb78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0  d  e  f\n",
       "row_0         \n",
       "a      1  0  0\n",
       "b      0  1  0\n",
       "c      0  0  0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(foo, bar, dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e94be9-ae07-48a9-b3c2-b4dba0fb2eb6",
   "metadata": {},
   "source": [
    "#### Scalability\n",
    "Pandas is excellent for working with large datasets. Even with a typical laptop you can manipulate millions of rows of data and go well beyond the size limit of spreadsheets. There's no hard limit on data frame size. If the data is too large for your machine to handle, you can scale up to a machine with more memory and more processing power. If that's not an option, pandas has built-in functions to work with data in chunks. For example, the pandas .read_csv() function includes a parameter to set the chunk size. You can use this parameter in combination with other functions and Python code to conserve memory. Even when you reach a limit with Pandas alone, you can combine it with other packages to take advantage of distributed computing and parallel processing. Handling datasets with hundreds of millions of rows is quite possible!\n",
    "\n",
    "- No hard limits on data frasme size\n",
    "- Built-in ways to \"chunk\" data\n",
    "- Use distributed/parallel computing\n",
    "\n",
    "#### Efficiency\n",
    "Pandas can also save you time when joining data. You can join datasets by any number of columns if the data logically matches. For instance, if you want to join data by month and day, and both datasets contain those columns, you can join directly on the columns. There's no need to create a new column for the date or combine text columns as you might for a spreadsheet. The code behind pandas is written to make things as easy as possible. When two data frames have the same names for overlapping columns, the statement can be super simple. This merge statement joins two data frames by their common columns and common row indexes. We refer to the first data frame mentioned as the left data frame, and the data frame inside the parentheses as the right data frame.\n",
    "\n",
    "- Join on multiple columns\n",
    "- Perference for simple code\n",
    "- `joined_df = left_df.merge(right_df)`\n",
    "\n",
    "#### Integration\n",
    "Finally, pandas is well-integrated into the python ecosystem. You heard earlier about packages designed to improve the speed and scale of manipulating data frames. Pandas also works well as part of an end-to-end pipeline for analytics. Other packages, especially those focused on visualization or machine learning, are written to accept pandas data frames as inputs.\n",
    "- Improved speed and scale\n",
    "- Data Visualization\n",
    "- Machine learning\n",
    "\n",
    "#### A word on advanced spreadsheet usage\n",
    "Of course, modern spreadsheet software has advanced capabilities that make it powerful, too. Data models and query tools allow users to join data in different ways. Integration with programming languages can populate cells with the touch of a button. And formulas using XLOOKUP or index-match have great flexibility for joining data. For this course we'll still use VLOOKUP as the baseline for joining data in spreadsheets. It's a nearly universal formula familiar to spreadsheet users at all levels. And, it provides a simple, useful concept for building joins in Pandas.\n",
    "\n",
    "- Data models and query tools\n",
    "- Programing languages\n",
    "- Advanced formulas\n",
    "\n",
    "#### Types of joins\n",
    "The three types are known as one-to-one, one-to-many, and many-to-many. These names describe the relationship of one table to another based on a lookup column, also known as a key column or index. In a spreadsheet environment, the values in the key column are like the lookup values in a VLOOKUP formula.\n",
    "\n",
    "- Types\n",
    "    - One-to-one\n",
    "    - One-to-many\n",
    "    - Many-to-many\n",
    "- Join type indicates relationship of tables\n",
    "- Like lookup values in VLOOKUP\n",
    "\n",
    "##### One-to-one\n",
    "We can use our first type of join, the one-to-one join, for datasets at the same level. In other words, the lookup column of each table is the same for both tables.\n",
    "- Datasets at the same level\n",
    "- Lookup column same for both tables\n",
    "- Rows match one-to-one\n",
    "\n",
    "##### One-to-many\n",
    "One-to-many joins are slightly more complex. They apply when tables have unique rows of data at different levels.\n",
    "\n",
    "- Rows are unique at different levels\n",
    "- Join based on the higher-level column\n",
    "- Result basedf on lower-level column\n",
    "- Simular to VLOOKUP based on lower-level data\n",
    "\n",
    "##### Many-tomany\n",
    "The last type of join, many-to-many, is often the most difficult to treat. The many-to-many join involves datasets with repeated values in the key column for both tables.\n",
    "- Values are repeated\n",
    "- Results in all possible combinations\n",
    "- Can be hard to interpret\n",
    "- Best supplemented with a \"bridge\" table\n",
    "\n",
    "#### Basics of pandas.merge()\n",
    "- Used with compementary data\n",
    "- `pd.merge(left, right)`\n",
    "- Joins on one or more columns\n",
    "- Similar to VLOOKUP\n",
    "\n",
    "##### Left merges\n",
    "- `pd.merge(df_left, df_right, on='GameKey', how='left')`\n",
    "    - List data frame names for \"df_left\" and \"df_right\" placeholders\n",
    "    - Resulting frame matches rows in left frame\n",
    "- `pd.merge(df_left, df_right, on='GameKey', how='right')`\n",
    "    - Same concept as left merge\n",
    "    - Resulting frame rows match right frame\n",
    "\n",
    "##### A framework for joins\n",
    "After viewing and understanding the data:\n",
    "- Determine the relationship\n",
    "- Check for unique values in key column\n",
    "- Write merge statement and execute\n",
    "\n",
    "##### Joining on two keys\n",
    "- Determine the relationship\n",
    "- Check for unique values in key column\n",
    "- Write merge statement and execute\n",
    "- `pd.merge(df_left, df_right, on=['GameKey', 'PlayId'])`\n",
    "\n",
    "##### Example\n",
    "##### # Merge data frames\n",
    "plays = pd.merge(play_times, play_yards, on=['GameId', 'PlayId'])\n",
    "\n",
    "##### # Check for duplicated values in key columns\n",
    "plays.duplicated(['GameId', 'PlayId']).sum()\n",
    "\n",
    "#### Combinging common data with inner joins\n",
    "- **Object-orinted expressions:**\n",
    "    - `pd.merge(df_left, df_right, on=None)`\n",
    "- **Shorthand, pandas dataframe method:**\n",
    "    - `df_left.merge(df_right, on=None)`\n",
    "    \n",
    "##### A basic inner join\n",
    "- Similar to VLOOKUP followed by removing NA's\n",
    "    - `df1.merge(df2, on='GameKey', how='inner')`\n",
    "- Returns only rows present in both tables\n",
    "\n",
    "##### Joining on different names\n",
    "- Key columns may have different names\n",
    "- No need to rename columns\n",
    "    - `df1.merge(df2, left_on='GameKey', right_on='game-key', how='inner')`\n",
    "    - The resulting data frame will use the column name from the left-hand table.\n",
    "    \n",
    "##### Example\n",
    "##### # Recreate merge statement\n",
    "thursday_impacts = pd.merge(thursdays, impacts, on='GameKey', how='left')\n",
    "\n",
    "##### # Rewrite merge expression\n",
    "thursday_impacts_oo = thursdays.merge(impacts, on='GameKey', how='left')\n",
    "\n",
    "##### # Compare data frames\n",
    "print(thursday_impacts.equals(thursday_impacts_oo))\n",
    "<br> **Boolean result** \n",
    "\n",
    "#### \"Out of many, one\"\n",
    "If you look on the back of US currency, you'll see the Latin words \"E Pluribus Unum\" which means \"Out of Many, one. \n",
    "\n",
    "##### One-to-many joins\n",
    "- One row matches to many rows\n",
    "- Resulting structure mirrors the lower level\n",
    "- After viewing and understanding the data:\n",
    "    - Detyermine the relationship\n",
    "- Relationship examples\n",
    "    - Time relationships\n",
    "        - Month and days\n",
    "        - Hour and minutes\n",
    "    - Hierachy relationships\n",
    "        - Company and departments\n",
    "        - Nations and cities\n",
    "    - Different entities\n",
    "        - Customer and purchase orders\n",
    "        - Department descriptions and expense breakdown\n",
    "        \n",
    "#### Joining on key columns\n",
    "- **Unique key columns**\n",
    "    - Unique values for single/multi column key\n",
    "        - `df.duplicated('GameKey').sum()`\n",
    "        - `df.duplicated(['GameKey','PlayId').sum()`\n",
    "    - Full syntax:\n",
    "        - `DataFrame.merge(right, how='inner', on=None, left_on=None, righ_on=None, left_index=False, right_index=False, sort=False, suffixes=('_x', '_y'), copy=True, indicator=False, validate=None)`\n",
    "        - **Validating merges**\n",
    "            - Values for `validate`:\n",
    "                - \"one_to_one\" or \"1:1\"\n",
    "                - \"one_to_many\" or \"1:m\"\n",
    "                - \"many_to_one\" or \"m:1\"\n",
    "                - \"many_to_many\" or \"m:m\" (does nothing)\n",
    "\n",
    "#### Index-based joins\n",
    "- We can join data frames based on either generic or tailored indexes. The data frame on the left has a generic index starting with 0, which is the default for pandas data frames. It can be most easily joined to another data frame with a similar default-like index. The frame on the right has a customized multi-level index. \n",
    "- To join frames based on index, we can use the pandas data frame join method. The join method is similar to the merge method with some exceptions. It joins frames on index by default, so you don't need to specify an 'on' column. Also, you can join multiple data frames at once with the join method. Just pass a bracketed list of data frames as the 'other' frames. This feature is very useful when importing and joining many files with similar indexes.\n",
    "- Joining on index\n",
    "    - `DataFrame.join(other, how='')`\n",
    "    - Similar to merge on columns\n",
    "    - Joins on index by default\n",
    "    - Can join multiple data frames\n",
    "    - `df.join([df2,df3], how='')`\n",
    "    \n",
    "#### Joining data in real life\n",
    "    - Mixing columns and indexes\n",
    "        - 'df1.merge(df2, left_on='column_one', right_index=True)`\n",
    "        - 'df1.merge(df2, left_index=True, right_on='column_two')`\n",
    "    - Working with overlapping column names\n",
    "        - df1.merge(df2, one='column_one', suffixes=('_df1', '_df2))\n",
    "        - df1.merge(df2, how='outer', on='column_one', suffixes=(_df1','_df2'), indicator=True)\n",
    "        - df1.merge(df2, one='column_one', suffixes=('_df1', '_df2), sort=True)\n",
    "\n",
    "#### Working with time data\n",
    "- **Joining with .merge_ordered()**\n",
    "    - Pandas has a special function called merge_ordered. It behaves similarly to pandas merge for ordered data such as dates. Calling the merge_ordered function performs an outer merge by default. Note this is different from regular pandas merge, which defaults to an inner merge. It’s not a hard constraint though - you can use the ‘how’ parameter to change it.\n",
    "    - `pd.merge_ordered(df1, df2, on='column_date', suffixes=['_df1','_df2'])`\n",
    "- **Interpolating data**\n",
    "    - `pd.merge_ordered(df1, df2, on='column_date', suffixes=['_df1','_df2'], fill_method='ffill')` 'ffill' = forward fill - the last observed value and carries forward\n",
    "- **Merging to nearest date-times**\n",
    "    - pandas.merge_asof()\n",
    "    - matches on nearest date\n",
    "    - similar to VLOOKUP(range_lookup=True)\n",
    "    - `pd.nerge_asof(left_df, right_df, direction='backward')`\n",
    "        - **Direction**\n",
    "            - There are three options. backward forward and nearest. The default value, 'backward', selects the row in the right-hand data frame with the closest date that is earlier than the target date. 'Forward' selects the closest row in the right data frame whose date is on or later than the target date. And 'Nearest' selects the row in the right Data Frame whose date is closest to the target date, regardless of time order.\n",
    "            - \"backword\": cloest date that is earlier\n",
    "            - \"foward\": cloest date equal or later\n",
    "            - \"nearest\": cloest date regardless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba46f57-f813-4a0f-917d-f24cd5d7649a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
