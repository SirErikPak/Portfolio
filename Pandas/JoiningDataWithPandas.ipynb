{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9988925-845d-42e2-a0da-e92fcdc7059b",
   "metadata": {},
   "source": [
    "# Joining Data with pandas\n",
    "- The city of Chicago is divided into fifty local neighborhoods called wards. We have a table with data about the local government offices in each ward. In this example, we want to merge the local government data with census data about the population of each ward. \n",
    "\n",
    "##### One-to-one\n",
    "- In a one-to-one relationship, every row in the left table is related to one and only one row in the right table.\n",
    "\n",
    "##### One-to-many\n",
    "- in a one-to-many relationship, every row in the left table is related to one or more rows in the right table.\n",
    "\n",
    "##### Merging multiple DataFrames\n",
    "- df1.merge(df2, on='col') \\\n",
    " &emsp;&emsp;     merge(df3, on='col')\n",
    "- df1.merge(df2, on='col') \\\n",
    " &emsp;&emsp;     merge(df3, on='col') \\\n",
    " &emsp;&emsp;     merge(df4, on='col')\n",
    "\n",
    "##### Merging a table to itself or self join\n",
    "- To complete this merge, we set the sequels table as input to the merge method for both the left and right tables. We can think of it as merging two copies of the same table. All of the aspects we have reviewed regarding merging two tables still apply here. Therefore, we can merge the tables on different columns. We'll use the 'left_on' and 'right_on' attributes to match rows where the sequel's id matches the original movie's id. Finally, setting the suffixes argument in the merge method allows us to identify which columns describe the original movie and which describe the sequel.\n",
    "\n",
    "##### Merging on indexes\n",
    "- df_merge_index = df1.merge(df2, on = 'column_name', how='left)\n",
    "\n",
    "##### MultiIndex datasets\n",
    "- df_multiIndex = df1.merge(df2, on=['column_1','column_2'])\n",
    "- df_multiIndex = df1.merge(df2, left_on=$\\color{red}{\\text{'column_1'}}$, left_index=$\\color{red}{\\text{True}}$, right_on=$\\color{red}{\\text{'column_2'}}$, right_index=$\\color{red}{\\text{True}}$) \n",
    "\n",
    "#### Filtering joins\n",
    "- **Mutating joins:**\n",
    "    - Combines data from two tables based on matching observations in both tables\n",
    "- **Filtering joins:**\n",
    "    - Filter observations from table based on whether or not they match an observation in other table\n",
    "- **Semi-joins**\n",
    "    - Returns the intersection, similar to an inner joinb\n",
    "    - Return only columns from the left table and **not** the right\n",
    "    - No duplicates\n",
    "- **Anto-join:**\n",
    "    - Returns the left table, excluding the intersection\n",
    "    - Returns only columns from the left table and **not** the right\n",
    "- **Example**    \n",
    "###### # Merge employees and top_cust\n",
    "empl_cust = employees.merge(top_cust, on='srid', \n",
    "<br> &emsp;&emsp;             how='left', indicator=True)\n",
    "\n",
    "###### # Select the srid column where _merge is left_only\n",
    "srid_list = empl_cust.loc[empl_cust['_merge'] == 'left_only', 'srid']\n",
    "\n",
    "###### # Get employees not working with top customers\n",
    "print(employees[employees['srid'].isin(srid_list)])\n",
    "\n",
    "##### Concatenate DataFrames together vertically\n",
    "- **Ignoring the index**\n",
    "    - If the index contains no valuable information, then we can ignore it in the concat method by setting ignore_index to True. The result is that the index will go from 0 to n-1.\n",
    "    - `pd.concat([df1, df2, df3], ignore_index=True)`\n",
    "    \n",
    "##### Setting labels to original tables\n",
    "- Now, suppose we wanted to associate specific keys with each of the pieces of our three original tables. We can provide a list of labels to the keys argument. Make sure that ignore_index argument is False, since you can't add a key and ignore the index at the same time. This results in a table with a multi-index, with the label on the first level.\n",
    "- `pd.concat([df_1, df_2, df_3], ignore_index=False, keys= ['column_1','column_2','column_3'])`\n",
    "\n",
    "##### Concatenate DataFrames tables with different column names\n",
    "- **Concatenate tables with different column names**\n",
    "- The concat method by default will include all of the columns in the different tables it's combining. The sort argument, if true, will alphabetically sort the different column names in the result.\n",
    "- `pd.concat([inv_jan, inv_feb], sort=True)` \n",
    "- **Concatenate tables with different column names**\n",
    "- If we only want the matching columns between tables, we set the join argument to \"inner\". Its default value is equal to \"outer\", which is why concat by default will include all of the columns. Additionally, the sort argument has no effect when join equals \"inner\". The order of the columns will be the same as the input tables. \n",
    "- `pd.concat([df_1, df_2], join='inner')`\n",
    "- Using append method\n",
    "- `.append()`\n",
    "- Simplified version of the `.concat()` method\n",
    "- Suppoorts: `ignore_index`, and `sort`\n",
    "- Does Not Support: `keys` and `join`\n",
    "    - Always `join = outer`\n",
    "- **Append the tables**\n",
    "- df_1.append([df_2, df_3], ignore_index=$\\color{red}{\\text{True}}$, sort=$\\color{red}{\\text{True}}$)\n",
    "\n",
    "- **Validating merges\n",
    "<br>`.merge(validate=None)`:\n",
    "    - Checks if merge is of specific type\n",
    "        - `one_to_one`\n",
    "        - `one_to_many`\n",
    "        - `many_to_many`\n",
    "        - `manymerge_ordered() caution, multiple columnsmerge_ordered() caution, multiple columnsmerge_ordered() caution, multiple columns\n",
    "- **Verifying concatenations**\n",
    "<br> `.concat(verify_integrity=Flase)`:\n",
    "    - Check whether the new concatenated index contains duplicates\n",
    "    - Default value is `False`\n",
    "\n",
    "- **merge_ordered() caution, multiple columns**\n",
    "    - When using merge_ordered() to merge on multiple columns, the order is important when you combine it with the forward fill feature. \n",
    "    \n",
    "- **Using merge_asof()**\n",
    "    - Similar to a `merge_ordered()` left-join\n",
    "        - Similar to a `merge_ordered()` \n",
    "    - Match on the nearest key column and not exact matches.\n",
    "        - Merged \"on\" columns mudst be sorted.\n",
    "        \n",
    "- Selecting data with .query()\n",
    "<br>`.query('SOME SELECTION STATEMENT')`\n",
    "    - Accepts an input string\n",
    "        - Input string to determine what rows are returned\n",
    "        - Input string similar to statement after **WHERE** cluse in **SQL** statementy\n",
    "            - Prior knowledge of SQL is not necessary\n",
    "            - `df.query('column_name >= 90')`\n",
    "            - `df.query('column_1 > 90 and columnn_2 < 140')`\n",
    "            - `df.query('column_1 > 96 or columnn_2 < 98')`\n",
    "\n",
    "##### using .query() to select text\n",
    "- We are interested in selecting all of the rows were the column stock equals \"disney\" or the column stock equals \"nike\" and close is less than 90. Let's pause here for a moment to look at our query string. Within the parentheses of our string, we check if the stock column is nike and the close column is less than 90. Both of these conditions have to be true for the parentheses section to return true. We then add that to the condition to check if stock is listed as \"disney\". When checking text, we use the double equal signs, similar to an if statement in Python. Also, when checking a text string, we used double quotes to surround the word. This is to avoid unintentionally ending our string statement since we used single quotes to start the statement. \n",
    "- `stocks_long.query('stock == \"disney\" or (stock == \"nike\" and close < 90)')`\n",
    "\n",
    "##### Reshaping data with .melt()\n",
    "- **Wide versus long data**\n",
    "    - Sometimes we will come across data where every row relates to one subject, and each column has different information about an attribute of that subject. Data formatted in this way is often called wide. There are other times when the information about one subject is found over many rows, and each row has one attribute about that subject. Data formatted in this way is often called long or tall. In general, wide formatted data is easier to read by people than long formatted. However, long formatted data is often more accessible for computers to work with.\n",
    "\n",
    "- **What does the .melt() method do?**\n",
    "    - The melt method will allow us to unpivot, or change the format of, our dataset. In this image, we change the height and weight columns from their wide horizontal placement to a long vertical placement.\n",
    "- The melt method will allow us to unpivot dataset\n",
    "\n",
    "- **Example of .melt()**\n",
    "    - Here we call the melt() method on the table social_fin. The first input argument to the method is id_vars. These are columns to be used as identifier variables. We can also think of them as columns in our original dataset that we do not want to change.\n",
    "    - social_fin_tall = social_fin.melt(id_vars=['financials','comapny'])\n",
    "    - **Melting with value_vars**\n",
    "        - This time, let's use the argument value_vars with the melt() method. This argument will allow us to control which columns are unpivoted. This time, let's use the argument value_vars with the melt() method. This argument will allow us to control which columns are unpivoted. Here, we unpivot only the 2018 and 2017 columns. Our output now only has data for the years 2018 and 2017. Additionally, the order of the value_var was kept. The output starts with 2018, then moves to 2017. Finally, notice that the column with the years is now named variable, and our values column is named value.\n",
    "            - social_fin_tall = social_fin.melt(id_vars=['financials','comapny'], value_vars=['2018','2017'])\n",
    "        - **Melting with column names**\n",
    "            - In this example, we have added some additional inputs to our melt() method. The var_name argument will allow us to set the name of the year column in the output. Similarly, the value_name argument will allow us to set the name of the value column in the output. It is the same as before, except our variable and value columns are renamed year and dollars, respectively. We have seen how the melt() method is useful for reshaping our tables. Imagine a situation where you have merged many columns, making your table very wide. The merge() method can then be used to reshape that table into a more computer-friendly format.\n",
    "            - social_fin_tall = social_fin.melt(id_vars=['financials','comapny'], value_vars=['2018','2017'],\n",
    "            <br>&emsp;&emsp;  var_name['year'], value_name='dollarts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0c236c-28ea-4c36-8519-3ba7fba4babf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
